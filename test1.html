<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>API TO SHARE PRODUCTS</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
</head>
<body>
  <div class="share-container">
    <button id="btn-share">Share</button>
  </div>
  
<script>
  window.addEventListener('load', function() {
    if(!navigator.share) {
      document.querySelector('.share-container').innerHTML = 'your browser does not support sharing';
      return;
    }

    document.getElementById('btn-share').addEventListener('click', function() {
      navigator.share({
        title: 'Check out this watch !',
        text: 'Its really cool',
        url: 'https://www.google.co.in',
      });
    });
  });

</script>
</body>
</html>




Path planning and Simulation for automatic multidose drug dispensing system

Introduction :

The automatic multi dose drug dispensing system is a pharmacy service which provides patient specific medicines prescribed by the doctor in the pill boxes. The system consists of various stations to perform different tasks and cars to carry the pill boxes upon which different operations are performed. A car moves to the individual stations to perform a particular task. Multiple cars can be in action simultaneously.  To carry out ease of operations in a time saving and cost effective manner, an optimal path planning of the cars is very crucial. The system is designed to perform multi-tasking by means of the cars, where in multiple cars can be into effect simultaneously but only one car taking an action at a time. We have used Reinforcement Learning for optimal path planning on square grids. Reinforcement Learning algorithm of Q - learning robustly learns to achieve the goal on small blocks and performs well on unseen situations too.  

Q-Learning : 

Q-Learning is a form of model free Reinforcement Learning. It provides agents with the capability of learning to act optimally in Markovian domains by experiencing the consequences of actions, without requiring them to build maps of the domains. In Q - Learning, an agent tries an action at a particular state and evaluates its consequences in terms of reward or penalty and its estimate of the value of the state it is taken to.  By trying all actions in all states repeatedly, it learns which are best overall, judged by long-term discounted reward.

Environment Overview (Scalability) :

An environment is the area of action where the agent lives. Basically an RL does not know anything about the environment, it learns what to do by exploring the environment. It uses actions, and receives states and rewards. Here, the environment is a (3 x 7) block with 2 floors. To reduce the number of training states, it is further divided into six (3 x 3) blocks. All the possible entrances and destinations are defined in the environment. Boxes where the entry of a car is prohibited are marked as no box. 


Data Preprocessing (according to the environment, it should explain state encoding and decoding along with q-values file structure) :

When multiple cars are active at the same time, the car which is performing the action at a given time is called Primary Car and others are called Secondary Cars. The state is encoded with primary and secondary car sources and destinations. Source and destination are separated by delimiter ‘x’ and cars are separated by the delimiter ‘z’. The primary car source and destination, comprising of Floor, box no. and orientation is written first followed by secondary car source and destination comprising of floor and box no. While decoding, the cars are splitted using the delimiter ‘z’ and source and destinations by the delimiter ‘x’. Q values are used to choose the action to be performed. They are stored in a file for each block while training. Q values are encoded with sources and destinations of cars, action performed and its Q value, separated by the delimiter ‘|’. Car source and destinations encoding is same as that in state, except  that the floor is represented by an alphabet. 


Algorithm Structure and Overview  (with flow chart)
- Exploration and Exploitation
- Reward function
- Deadlock detection










Model Validation :



Applications

Future Aspects


